{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge,LinearRegression,ElasticNet,Lasso\n",
    "from yellowbrick.regressor import prediction_error,residuals_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_2017_LPV_rows = [[1.59009400e+06, 1.61762700e+06, 1.43115413e+01, 2.84522000e+05, 2.54697000e+05,\n",
    "                        4.53646000e+05, 2.51078000e+05, 3.46148000e+05, 2.89449000e+05, 2.59108000e+05,\n",
    "                        4.61501000e+05, 2.55426000e+05, 3.52141000e+05],\n",
    "                       [284522.0, 0.0, 2.975101564, 284522.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [254697.0, 0.0, 7.975101564, 0.0, 254697.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [453646.0, 0.0, 12.97510156, 0.0, 0.0, 453646.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [251078.0, 0.0, 17.97510156, 0.0, 0.0, 0.0, 251078.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [346148.0, 0.0, 22.97510156, 0.0, 0.0, 0.0, 0.0, 346148.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 289449.0, 9.130000000, 0.0, 0.0, 0.0, 0.0, 0.0, 289449.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 259108.0, 14.13000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 259108.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 461501.0, 19.13000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 461501.0, 0.0, 0.0],\n",
    "                       [0.0, 255426.0, 24.13000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 255426.0, 0.0],\n",
    "                       [0.0, 352141.0, 29.13000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 352141.0], ]\n",
    "\n",
    "split_2017_LCV_rows = [[4.76173000e+05, 1.06367000e+05, 1.25145441e+01, 8.52030000e+04, 7.62720000e+04,\n",
    "                        1.35849000e+05, 7.51880000e+04, 1.03658000e+05, 1.90320000e+04, 1.70370000e+04,\n",
    "                        3.03460000e+04, 1.67950000e+04, 2.31550000e+04],\n",
    "                       [085203.0, 0.0, 2.868356783, 085203.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [076272.0, 0.0, 7.868356783, 0.0, 076272.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [135849.0, 0.0, 12.86835678, 0.0, 0.0, 135849.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [075188.0, 0.0, 17.86835678, 0.0, 0.0, 0.0, 075188.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [103658.0, 0.0, 22.86835678, 0.0, 0.0, 0.0, 0.0, 103658.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 019032.0, 07.98000000, 0.0, 0.0, 0.0, 0.0, 0.0, 019032.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 017037.0, 12.98000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 017037.0, 0.0, 0.0, 0.0],\n",
    "                       [0.0, 030346.0, 17.98000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 030346.0, 0.0, 0.0],\n",
    "                       [0.0, 016795.0, 22.98000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 016795.0, 0.0],\n",
    "                       [0.0, 023155.0, 27.98000000, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 023155.0], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_average_vehicle(file_dir):\n",
    "    average_vehicle_df = pd.read_excel(file_dir, sheet_name=\"2.1, 2.2, 2.3,2.4\", header=2, nrows=19, usecols=\"A:AH\")\n",
    "    return average_vehicle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_light_fleet_age(file_dir):\n",
    "    light_fleet_age_df = pd.read_excel(file_dir, sheet_name=\"2.10\", header=1, nrows=7)\n",
    "    return light_fleet_age_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_co2_emission(file_dir):\n",
    "    co2_emission_df = pd.read_excel(file_dir, sheet_name=\"1.10\", header=2, nrows=17, usecols=\"A:E\")\n",
    "    return co2_emission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_array_index(array):\n",
    "    print(np.sort(array))\n",
    "    order = []\n",
    "    for element in np.sort(array):\n",
    "        for idx, pca_value in enumerate(array):\n",
    "            if element == pca_value:\n",
    "                order.append(idx)\n",
    "\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration(column,column_name):\n",
    "    column = np.array(column)\n",
    "    min, max = np.nanmin(column), np.nanmax(column)\n",
    "    variance, std = np.nanvar(column), np.nanstd(column)\n",
    "    print('column: %s, range: [%.2f, %.2f] variance: %.2f +/- %.2f'%(column_name, min,max,variance, std) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_4_1_imputation_co2_emission_df(co2_emission_df):\n",
    "    missing_values = [[2000] + [np.nan for i in range(4)], [2018] + [np.nan for i in range(4)]]\n",
    "\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp.fit(co2_emission_df)\n",
    "    X = imp.transform(missing_values)\n",
    "\n",
    "    df = pd.DataFrame(X, columns=co2_emission_df.columns)\n",
    "    co2_emission_df = pd.concat([co2_emission_df, df])\n",
    "    co2_emission_df = co2_emission_df.sort_values(by=['Year'])\n",
    "    co2_emission_df.index = [x for x in range(len(co2_emission_df.index))]\n",
    "\n",
    "    return co2_emission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality(target, name):\n",
    "    mean,std = np.mean(target), np.std(target)\n",
    "    X = np.linspace(np.min(target), np.max(target), 1000)\n",
    "    pdf = stats.norm.pdf(X, mean, std)\n",
    "    plt.plot(X, pdf, label=\"PDF\")\n",
    "    plt.grid()\n",
    "    plt.title('Check Normal Distribution for %s' %name,fontsize=10)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_4_2_check_normality(number_fleets_df, light_fleet_age_df,co2_emission_df):\n",
    "    plot_normality(number_fleets_df['Total LPV new'], 'Total LPV new')\n",
    "    plot_normality(number_fleets_df[' Total LPV used'], 'Total LPV used')\n",
    "    plot_normality(number_fleets_df['Total LCV new'], 'Total LCV new')\n",
    "    plot_normality(number_fleets_df[' Total LCV used'], 'Total LCV used')\n",
    "\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[0][1:20].astype(int)), '0-4 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[1][1:20].astype(int)), '5-9 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[2][1:20].astype(int)), '10-14 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[3][1:20].astype(int)), '15-19 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[4][1:20].astype(int)), '20+ age group')\n",
    "\n",
    "\n",
    "    plot_normality(co2_emission_df['Light passenger'], 'Light passenger co2 emssion')\n",
    "    plot_normality(co2_emission_df['Light commercial'], 'Light commercial co2 emission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_1_clean_light_age_distribution(light_fleet_age_df):\n",
    "    light_fleet_age_df = light_fleet_age_df.T\n",
    "    light_fleet_age_df.index = [x for x in range(len(light_fleet_age_df.index))]\n",
    "\n",
    "    numbers = pd.DataFrame(light_fleet_age_df[1:20])\n",
    "    numbers = numbers.drop(columns=[6])\n",
    "    numbers.columns = ['0-4 years', '5-9 years', '10-14 years', '15-19 years', '20+ years', 'Total']\n",
    "    numbers.index = [i for i in range(2000, 2019)]\n",
    "\n",
    "    percentages = pd.DataFrame(light_fleet_age_df[20:])\n",
    "    percentages = percentages.drop(columns=5)\n",
    "    percentages.columns = ['0-4 years percentage', '5-9 years  percentage',\n",
    "                           '10-14 years percentage', '15-19 years percentage',\n",
    "                           '20+ years percentage', '15+ years percentage']\n",
    "    percentages.index = [i for i in range(2000, 2019)]\n",
    "\n",
    "    new_age_distribution = pd.concat([numbers, percentages], axis=1, join='inner')\n",
    "    new_age_distribution.insert(0, 'Period', new_age_distribution.index)\n",
    "    new_age_distribution.index = [i for i in range(len(new_age_distribution.index))]\n",
    "\n",
    "    return new_age_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_3_construct_new_distribution_df(nums_columns, percentage_columns, number_fleets_df, new_age_distribution_df):\n",
    "\n",
    "    # new_table = {'Period':[i for i in range(2000,2019)]} for internal output\n",
    "    new_table = {}\n",
    "    for num_column in nums_columns:\n",
    "        for percenate_column in percentage_columns:\n",
    "            new_column = new_age_distribution_df[percenate_column] * number_fleets_df[num_column]\n",
    "            new_column_name = '%s of %s' % (percenate_column[:-11].strip(), num_column[6:].strip())\n",
    "            new_table[new_column_name] = new_column\n",
    "\n",
    "    new_age_distribution_df = pd.DataFrame(new_table)\n",
    "\n",
    "    return new_age_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_5_convert_object_to_int(data_df):\n",
    "    for column in data_df.columns:\n",
    "        if data_df.dtypes[column] != np.float64:\n",
    "            data_df[column] = data_df[column].astype(np.int64)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_1_LPV_cols(data_df, printed=True):\n",
    "    default_LPV_cols = ['Period', 'Total LPV new',' Total LPV used', 'Light passenger average age',\n",
    "                        '0-4 years of LPV new', '5-9 years of LPV new', '10-14 years of LPV new',\n",
    "                        '15-19 years of LPV new', '20+ years of LPV new', '15+ years of LPV new',\n",
    "                        '0-4 years of LPV used', '5-9 years of LPV used', '10-14 years of LPV used',\n",
    "                        '15-19 years of LPV used', '20+ years of LPV used', '15+ years of LPV used', ]\n",
    "\n",
    "    LPV = data_df[default_LPV_cols]\n",
    "    pca = PCA(n_components=len(LPV.columns))\n",
    "    pca.fit(LPV)\n",
    "    pca_values =pca.explained_variance_ratio_\n",
    "    if printed:\n",
    "        print(pca_values)\n",
    "        print(pca.components_[0:2])\n",
    "        print(pca_values[0] + pca_values[1])\n",
    "        get_sort_array_index(pca.components_[0])\n",
    "        get_sort_array_index(pca.components_[1])\n",
    "\n",
    "    reduced_LPV_cols = ['Total LPV new', ' Total LPV used', 'Light passenger average age',\n",
    "                '0-4 years of LPV new', '5-9 years of LPV new', '10-14 years of LPV new',\n",
    "                '15-19 years of LPV new', '20+ years of LPV new',\n",
    "                '0-4 years of LPV used', '5-9 years of LPV used', '10-14 years of LPV used',\n",
    "                '15-19 years of LPV used', '20+ years of LPV used', 'Light passenger']\n",
    "\n",
    "    return reduced_LPV_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_1_LCV_cols(data_df,printed=True):\n",
    "    default_LCV_cols = ['Period',  'Total LCV new', ' Total LCV used', 'Light commercial average age',\n",
    "                        '0-4 years of LCV new', '5-9 years of LCV new', '10-14 years of LCV new',\n",
    "                        '15-19 years of LCV new', '20+ years of LCV new', '15+ years of LPV new',\n",
    "                        '0-4 years of LCV used', '5-9 years of LCV used', '10-14 years of LCV used',\n",
    "                        '15-19 years of LCV used', '20+ years of LCV used', '15+ years of LPV used', ]\n",
    "\n",
    "\n",
    "    LCV = data_df[ default_LCV_cols]\n",
    "\n",
    "    pca = PCA(n_components=len(LCV.columns))\n",
    "    pca.fit(LCV)\n",
    "    pca_values =pca.explained_variance_ratio_\n",
    "    if printed:\n",
    "        print(pca_values)\n",
    "        print(pca.components_[0])\n",
    "        print(pca_values[0])\n",
    "        get_sort_array_index(pca.components_[0])\n",
    "\n",
    "    reduced_LCV_cols = ['Total LCV new', ' Total LCV used', 'Light commercial average age',\n",
    "                        '0-4 years of LCV new', '5-9 years of LCV new', '10-14 years of LCV new',\n",
    "                        '15-19 years of LCV new', '20+ years of LCV new',\n",
    "                        '0-4 years of LCV used', '5-9 years of LCV used', '10-14 years of LCV used',\n",
    "                        '15-19 years of LCV used', '20+ years of LCV used', 'Light commercial']\n",
    "    return reduced_LCV_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_2_normalization(data_df,LPV_cols,LCV_cols):\n",
    "    LPV_df, LCV_df = data_df[LPV_cols],data_df[LCV_cols]\n",
    "    LPV_data,LCV_data = normalize( LPV_df, axis=1, norm='l2'),normalize(LCV_df, axis=1, norm='l2')\n",
    "    LPV_df, LCV_df = pd.DataFrame(LPV_data, columns= LPV_cols),pd.DataFrame(LCV_data, columns= LCV_cols)\n",
    "    return LPV_df, LCV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_5_split_data_add_noisy_data(data_df, test_size):\n",
    "    noisy_x_row = pd.DataFrame({x: [0.0] for x in data_df.columns[:-1]})\n",
    "    second_last_row = data_df.iloc[-2]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_df[data_df.columns[:-1]], data_df[data_df.columns[-1]], test_size=test_size)\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  pd.concat([X_train, noisy_x_row]),pd.concat([X_test, noisy_x_row]), \\\n",
    "                                        y_train.append(pd.Series([0.0])),y_test.append(pd.Series([0.0]))\n",
    "\n",
    "    X_train, y_train = X_train.append(pd.Series(second_last_row[:-1])), y_train.append(pd.Series(second_last_row[-1]))\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regressor(X_train,y_train, X_test,y_test):\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    return 'r2 score = %f,' %(regressor.score(X_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regressor(X_train,y_train, X_test,y_test, solver):\n",
    "    regressor = Ridge(solver=solver)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    return regressor.predict(X_test), ('r2 score = %f,' %(regressor.score(X_test, y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_6_select_alogorithms(X_train,X_test,y_train,y_test):\n",
    "    regressors = [LinearRegression(),Ridge(solver='sag'), Ridge(solver='saga'),Lasso(),ElasticNet()]\n",
    "    regressor_name = ['Ordinary Least Square Regression', 'Ridge with sag solver', 'Ridge with saga solver',\n",
    "                     'Lasso Regression', 'Elastic Net Regression']\n",
    "    \n",
    "    for idx,regressor in enumerate(regressors):\n",
    "        regressor.fit(X_train,y_train)\n",
    "        print('%s: r2 score = %f,' %(regressor_name[idx] ,regressor.score(X_test, y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_6_select_ridge(X_train,X_test,y_train,y_test):\n",
    "    regressors = [Ridge(solver='sag'), Ridge(solver='saga')]\n",
    "    regressor_name = [ 'Ridge with sag solver', 'Ridge with saga solver']\n",
    "    for idx,regressor in enumerate(regressors):\n",
    "        regressor.fit(X_train,y_train)\n",
    "        print('%s: r2 score = %f,' %(regressor_name[idx] ,regressor.score(X_test, y_test)) )\n",
    "        print( np.subtract(regressor.predict(X_test),y_test.values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_7_1_prepare(rows, columns):\n",
    "\n",
    "    test_obj_2 = {x:[] for _,x in enumerate(columns)}\n",
    "    for row in rows:\n",
    "        for idx, x in enumerate(columns):\n",
    "            test_obj_2[x].append(row[idx])\n",
    "\n",
    "    return pd.DataFrame(test_obj_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_2_visualizaiton(regressor, X_train, y_train, X_test, y_test):\n",
    "    predict_visualizer = prediction_error(regressor, X_train, y_train, X_test, y_test)\n",
    "    residual_viz = residuals_plot(regressor, X_train, y_train, X_test, y_test)\n",
    "    return predict_visualizer,residual_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_2_percentage_viz(percentage_list, columns):\n",
    "    fig = plt.figure(figsize=(9, 5.0625))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "\n",
    "    ratios = percentage_list\n",
    "    labels = columns\n",
    "    # rotate so that first wedge is split by the x-axis\n",
    "    angle = -180 * ratios[0]\n",
    "    ax1.pie(ratios, autopct='%1.1f%%', startangle=angle, labels=labels, )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
